{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models using bash file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sh train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning baseline RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python src/train.py --data_path \"./data\" \\\n",
    "            --model_name \"ixa-ehu/roberta-eus-euscrawl-large-cased\" \\\n",
    "            --fp16 True \\\n",
    "            --num_train_epochs 10 \\\n",
    "            --weight_decay 5e-5 \\\n",
    "            --learning_rate 5e-5 \\\n",
    "            --per_device_train_batch_size 32 \\\n",
    "            --per_device_eval_batch_size 4 \\\n",
    "            --gradient_accumulation_steps 1 \\\n",
    "            --eval_accumulation_steps 10 \\\n",
    "            --seed 42 \\\n",
    "            --auto_find_batch_size False \\\n",
    "            --optim \"adamw_torch\" \\\n",
    "            --lr_scheduler_type \"cosine\"\\\n",
    "            --save_total_limit 2 \\\n",
    "            --evaluation_strategy \"epoch\" \\\n",
    "            --save_strategy \"epoch\" \\\n",
    "            --logging_strategy \"steps\" \\\n",
    "            --logging_steps 25 \\\n",
    "            --metric_for_best_model \"f1\" \\\n",
    "            --load_best_model_at_end True \\\n",
    "            --output_dir \"./models/\" \\\n",
    "            --run_name \"finetuning-roberta-euscrawl\" \\\n",
    "            --do_train True \\\n",
    "            --do_eval True \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning using supervised contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 src/train.py --data_path \"./data/\" \\\n",
    "            --model_name \"ixa-ehu/roberta-eus-euscrawl-large-cased\" \\\n",
    "            --contrastive True \\\n",
    "            --contrastive_lam 0.2 \\\n",
    "            --contrastive_temp 0.2 \\\n",
    "            --fp16 True \\\n",
    "            --num_train_epochs 10 \\\n",
    "            --weight_decay 5e-5 \\\n",
    "            --learning_rate 5e-5 \\\n",
    "            --per_device_train_batch_size 32 \\\n",
    "            --per_device_eval_batch_size 4 \\\n",
    "            --gradient_accumulation_steps 1 \\\n",
    "            --eval_accumulation_steps 10 \\\n",
    "            --seed 42 \\\n",
    "            --auto_find_batch_size False \\\n",
    "            --optim \"adamw_torch\" \\\n",
    "            --lr_scheduler_type \"cosine\"\\\n",
    "            --save_total_limit 2 \\\n",
    "            --evaluation_strategy \"epoch\" \\\n",
    "            --save_strategy \"epoch\" \\\n",
    "            --logging_strategy \"steps\" \\\n",
    "            --logging_steps 25 \\\n",
    "            --metric_for_best_model \"f1\" \\\n",
    "            --load_best_model_at_end True \\\n",
    "            --output_dir \"./models/\" \\\n",
    "            --run_name \"finetuning-roberta-euscrawl-scl\" \\\n",
    "            --do_train True \\\n",
    "            --do_eval True \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning using supervised contrastive learning and different pooling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min pooling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 src/train.py --data_path \"./data/\" \\\n",
    "                --model_name \"ixa-ehu/roberta-eus-euscrawl-large-cased\" \\\n",
    "                --pooling_strategy \"min\" \\\n",
    "                --contrastive True \\\n",
    "                --contrastive_lam 0.2 \\\n",
    "                --contrastive_temp 0.2 \\\n",
    "                --fp16 True \\\n",
    "                --num_train_epochs 10 \\\n",
    "                --weight_decay 5e-5 \\\n",
    "                --learning_rate 5e-5 \\\n",
    "                --per_device_train_batch_size 32 \\\n",
    "                --per_device_eval_batch_size 4 \\\n",
    "                --gradient_accumulation_steps 1 \\\n",
    "                --eval_accumulation_steps 10 \\\n",
    "                --seed 42 \\\n",
    "                --auto_find_batch_size False \\\n",
    "                --optim \"adamw_torch\" \\\n",
    "                --lr_scheduler_type \"cosine\"\\\n",
    "                --save_total_limit 2 \\\n",
    "                --evaluation_strategy \"epoch\" \\\n",
    "                --save_strategy \"epoch\" \\\n",
    "                --logging_strategy \"steps\" \\\n",
    "                --logging_steps 25 \\\n",
    "                --metric_for_best_model \"f1\" \\\n",
    "                --load_best_model_at_end True \\\n",
    "                --output_dir \"./models/\" \\\n",
    "                --run_name \"finetuning-roberta-euscrawl-scl-min_pooling\" \\\n",
    "                --do_train True \\\n",
    "                --do_eval True \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 src/train.py --data_path \"./data/\" \\\n",
    "                --model_name \"ixa-ehu/roberta-eus-euscrawl-large-cased\" \\\n",
    "                --pooling_strategy \"max\" \\\n",
    "                --contrastive True \\\n",
    "                --contrastive_lam 0.2 \\\n",
    "                --contrastive_temp 0.2 \\\n",
    "                --fp16 True \\\n",
    "                --num_train_epochs 10 \\\n",
    "                --weight_decay 5e-5 \\\n",
    "                --learning_rate 5e-5 \\\n",
    "                --per_device_train_batch_size 32 \\\n",
    "                --per_device_eval_batch_size 4 \\\n",
    "                --gradient_accumulation_steps 1 \\\n",
    "                --eval_accumulation_steps 10 \\\n",
    "                --seed 42 \\\n",
    "                --auto_find_batch_size False \\\n",
    "                --optim \"adamw_torch\" \\\n",
    "                --lr_scheduler_type \"cosine\"\\\n",
    "                --save_total_limit 2 \\\n",
    "                --evaluation_strategy \"epoch\" \\\n",
    "                --save_strategy \"epoch\" \\\n",
    "                --logging_strategy \"steps\" \\\n",
    "                --logging_steps 25 \\\n",
    "                --metric_for_best_model \"f1\" \\\n",
    "                --load_best_model_at_end True \\\n",
    "                --output_dir \"./models/\" \\\n",
    "                --run_name \"finetuning-roberta-euscrawl-scl-max_pooling\" \\\n",
    "                --do_train True \\\n",
    "                --do_eval True \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean pooling (Average pooling) strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 src/train.py --data_path \"./data/\" \\\n",
    "                --model_name \"ixa-ehu/roberta-eus-euscrawl-large-cased\" \\\n",
    "                --pooling_strategy \"mean\" \\\n",
    "                --contrastive True \\\n",
    "                --contrastive_lam 0.2 \\\n",
    "                --contrastive_temp 0.2 \\\n",
    "                --fp16 True \\\n",
    "                --num_train_epochs 10 \\\n",
    "                --weight_decay 5e-5 \\\n",
    "                --learning_rate 5e-5 \\\n",
    "                --per_device_train_batch_size 32 \\\n",
    "                --per_device_eval_batch_size 4 \\\n",
    "                --gradient_accumulation_steps 1 \\\n",
    "                --eval_accumulation_steps 10 \\\n",
    "                --seed 42 \\\n",
    "                --auto_find_batch_size False \\\n",
    "                --optim \"adamw_torch\" \\\n",
    "                --lr_scheduler_type \"cosine\"\\\n",
    "                --save_total_limit 2 \\\n",
    "                --evaluation_strategy \"epoch\" \\\n",
    "                --save_strategy \"epoch\" \\\n",
    "                --logging_strategy \"steps\" \\\n",
    "                --logging_steps 25 \\\n",
    "                --metric_for_best_model \"f1\" \\\n",
    "                --load_best_model_at_end True \\\n",
    "                --output_dir \"./models/\" \\\n",
    "                --run_name \"finetuning-roberta-euscrawl-scl-mean_pooling\" \\\n",
    "                --do_train True \\\n",
    "                --do_eval True \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum pooling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 src/train.py --data_path \"./data/\" \\\n",
    "                --model_name \"ixa-ehu/roberta-eus-euscrawl-large-cased\" \\\n",
    "                --pooling_strategy \"sum\" \\\n",
    "                --contrastive True \\\n",
    "                --contrastive_lam 0.2 \\\n",
    "                --contrastive_temp 0.2 \\\n",
    "                --fp16 True \\\n",
    "                --num_train_epochs 10 \\\n",
    "                --weight_decay 5e-5 \\\n",
    "                --learning_rate 5e-5 \\\n",
    "                --per_device_train_batch_size 32 \\\n",
    "                --per_device_eval_batch_size 4 \\\n",
    "                --gradient_accumulation_steps 1 \\\n",
    "                --eval_accumulation_steps 10 \\\n",
    "                --seed 42 \\\n",
    "                --auto_find_batch_size False \\\n",
    "                --optim \"adamw_torch\" \\\n",
    "                --lr_scheduler_type \"cosine\"\\\n",
    "                --save_total_limit 2 \\\n",
    "                --evaluation_strategy \"epoch\" \\\n",
    "                --save_strategy \"epoch\" \\\n",
    "                --logging_strategy \"steps\" \\\n",
    "                --logging_steps 25 \\\n",
    "                --metric_for_best_model \"f1\" \\\n",
    "                --load_best_model_at_end True \\\n",
    "                --output_dir \"./models/\" \\\n",
    "                --run_name \"finetuning-roberta-euscrawl-scl-sum_pooling\" \\\n",
    "                --do_train True \\\n",
    "                --do_eval True \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention pooling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 src/train.py --data_path \"./data/\" \\\n",
    "                --model_name \"ixa-ehu/roberta-eus-euscrawl-large-cased\" \\\n",
    "                --pooling_strategy \"attention\" \\\n",
    "                --contrastive True \\\n",
    "                --contrastive_lam 0.2 \\\n",
    "                --contrastive_temp 0.2 \\\n",
    "                --fp16 True \\\n",
    "                --num_train_epochs 10 \\\n",
    "                --weight_decay 5e-5 \\\n",
    "                --learning_rate 5e-5 \\\n",
    "                --per_device_train_batch_size 32 \\\n",
    "                --per_device_eval_batch_size 4 \\\n",
    "                --gradient_accumulation_steps 1 \\\n",
    "                --eval_accumulation_steps 10 \\\n",
    "                --seed 42 \\\n",
    "                --auto_find_batch_size False \\\n",
    "                --optim \"adamw_torch\" \\\n",
    "                --lr_scheduler_type \"cosine\"\\\n",
    "                --save_total_limit 2 \\\n",
    "                --evaluation_strategy \"epoch\" \\\n",
    "                --save_strategy \"epoch\" \\\n",
    "                --logging_strategy \"steps\" \\\n",
    "                --logging_steps 25 \\\n",
    "                --metric_for_best_model \"f1\" \\\n",
    "                --load_best_model_at_end True \\\n",
    "                --output_dir \"./models/\" \\\n",
    "                --run_name \"finetuning-roberta-euscrawl-scl-attention_pooling\" \\\n",
    "                --do_train True \\\n",
    "                --do_eval True \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC_lana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
